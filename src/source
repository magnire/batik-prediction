import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Conv2D, BatchNormalization, ReLU
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from PIL import Image, UnidentifiedImageError
import numpy as np
import matplotlib.pyplot as plt
from google.colab import files
import os
import math

# Define the class labels for our classification task.
CLASS_LABELS = ["bokor-kencono", "truntum"]

# Specify the directories where training and validation datasets are stored.
train_dir = "/content/drive/MyDrive/dataset/train"
validation_dir = "/content/drive/MyDrive/dataset/validation"

# Define a custom preprocessing function that applies a random hue shift.
# The input image is expected to have pixel values in the [0, 1] range.
def random_hue_shift(image):
    # Convert the numpy image array to a TensorFlow tensor of type float32.
    image_tensor = tf.convert_to_tensor(image, dtype=tf.float32)
    # Apply a random hue adjustment with a maximum delta of 0.1.
    image_tensor = tf.image.random_hue(image_tensor, max_delta=0.1)
    # Convert the tensor back to a numpy array and return it.
    return image_tensor.numpy()

# Create an ImageDataGenerator for training data augmentation.
# This generator applies several random transformations to increase data diversity.
train_augmentor = ImageDataGenerator(
    rotation_range=40,               # Randomly rotate images by up to 40 degrees.
    width_shift_range=0.2,           # Randomly shift images horizontally by up to 20% of the width.
    height_shift_range=0.2,          # Randomly shift images vertically by up to 20% of the height.
    shear_range=0.2,                 # Apply shear transformations by up to 20%.
    zoom_range=0.2,                  # Randomly zoom in/out by up to 20%.
    brightness_range=[0.8, 1.2],     # Randomly adjust brightness between 80% and 120%.
    horizontal_flip=True,            # Randomly flip images horizontally.
    fill_mode="nearest",             # Use nearest-neighbor filling for pixels created after a transformation.
    preprocessing_function=random_hue_shift  # Apply the custom random hue shift function after other transformations.
)

# For validation images, no augmentation is applied.
validation_augmentor = None

# Define a custom generator function that:
# - Walks through the directory structure
# - Loads images, applies center cropping and resizing
# - Optionally applies augmentation
# - Yields batches of images and their corresponding one-hot encoded labels.
def custom_flow_from_directory(directory, batch_size, classes, augmentor=None, shuffle=True, target_size=(224, 224)):
    """
    Custom generator that:
      - Walks through a directory structure.
      - Filters files by common image extensions.
      - Loads each image, applies a center crop to square, and resizes to target_size.
      - Applies augmentation if provided.
      - Yields batches of images and one-hot encoded labels.
    """
    valid_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.webp')  # Valid image file extensions.
    filepaths = []  # List to store full paths to image files.
    labels = []     # List to store the corresponding label index for each image.

    # Loop through each class label and get the files inside its directory.
    for idx, cl in enumerate(classes):
        class_dir = os.path.join(directory, cl)  # Construct the directory path for the current class.
        for fname in os.listdir(class_dir):      # Loop through all files in the class directory.
            # Skip files that do not have a valid image extension.
            if not fname.lower().endswith(valid_extensions):
                continue
            # Append the full file path and corresponding label index.
            filepaths.append(os.path.join(class_dir, fname))
            labels.append(idx)

    # Create an array of indices corresponding to the collected images.
    indices = np.arange(len(filepaths))
    if shuffle:
        np.random.shuffle(indices)  # Shuffle the indices if desired.

    # Infinite loop to continuously generate batches.
    while True:
        batch_images = []  # List to collect images for the current batch.
        batch_labels = []  # List to collect one-hot encoded labels for the current batch.
        for i in indices:
            fp = filepaths[i]  # Get the file path for the image.
            label = labels[i]  # Get the corresponding label index.
            try:
                # Load the image and convert it to RGB format.
                img = load_img(fp)
                img = img.convert("RGB")
            except (UnidentifiedImageError, OSError) as e:
                # If the image fails to load, print an error message and skip this file.
                print(f"Skipping file {fp} due to error: {e}")
                continue

            # Get the dimensions of the image.
            width, height = img.size
            # Determine the size of the square crop (the smaller of the two dimensions).
            new_edge = min(width, height)
            # Calculate coordinates for a center crop.
            left = (width - new_edge) // 2
            top = (height - new_edge) // 2
            right = left + new_edge
            bottom = top + new_edge
            # Crop the image to a centered square.
            img = img.crop((left, top, right, bottom))

            # Resize the cropped image to the target size using a high-quality filter.
            img = img.resize(target_size, Image.Resampling.LANCZOS)

            # Convert the image to a numpy array and normalize pixel values to [0, 1].
            img_array = img_to_array(img).astype(np.float32) / 255.0

            # If an augmentation function is provided, apply random transformations.
            if augmentor is not None:
                img_array = augmentor.random_transform(img_array)
                # If a preprocessing function (like random hue shift) is defined, apply it.
                if augmentor.preprocessing_function is not None:
                    img_array = augmentor.preprocessing_function(img_array)

            # Add the processed image to the batch.
            batch_images.append(img_array)
            # Convert the label to a one-hot encoded vector and add to the batch.
            batch_labels.append(tf.keras.utils.to_categorical(label, num_classes=len(classes)))

            # Once the batch reaches the specified size, yield the batch.
            if len(batch_images) == batch_size:
                yield np.array(batch_images), np.array(batch_labels)
                batch_images = []  # Reset the image list for the next batch.
                batch_labels = []  # Reset the label list for the next batch.

        # If there are any images left after processing all indices, yield them as a final batch.
        if batch_images:
            yield np.array(batch_images), np.array(batch_labels)

# Set the batch size for training.
batch_size = 32

# Create the generator for training data with augmentation and shuffling.
train_data = custom_flow_from_directory(train_dir, batch_size, CLASS_LABELS,
                                          augmentor=train_augmentor, shuffle=True, target_size=(224, 224))

# Create the generator for validation data without augmentation and without shuffling.
validation_data = custom_flow_from_directory(validation_dir, batch_size, CLASS_LABELS,
                                               augmentor=validation_augmentor, shuffle=False, target_size=(224, 224))

# Define a custom MixConv layer that applies multiple convolutions with different kernel sizes,
# then concatenates and normalizes their outputs.
class MixConv(tf.keras.layers.Layer):
    def __init__(self, filters, kernel_sizes=[3, 5, 7], strides=1, activation=True, **kwargs):
        # Initialize the parent Layer class.
        super(MixConv, self).__init__(**kwargs)
        self.kernel_sizes = kernel_sizes  # List of kernel sizes to use.
        self.strides = strides            # Stride for the convolution operations.
        self.activation = activation      # Whether to apply an activation function.

        num_kernels = len(kernel_sizes)   # Number of different kernel sizes.
        # Evenly distribute the total number of filters across the different kernels.
        channels_per_conv = [filters // num_kernels] * num_kernels
        for i in range(filters % num_kernels):
            channels_per_conv[i] += 1     # Distribute any remainder filters among the first few kernels.

        # Create a Conv2D layer for each kernel size with its allocated number of filters.
        self.conv_layers = [
            Conv2D(ch, (k, k), strides=strides, padding="same", use_bias=False)
            for ch, k in zip(channels_per_conv, kernel_sizes)
        ]
        self.batch_norm = BatchNormalization()  # Batch normalization to normalize the output.
        # Use ReLU activation if enabled; otherwise, set to None.
        self.relu = ReLU() if activation else None

    def call(self, inputs):
        # Apply each convolution layer to the input.
        conv_outputs = [conv(inputs) for conv in self.conv_layers]
        # Concatenate the outputs from all convolutions along the channel dimension.
        x = tf.concat(conv_outputs, axis=-1)
        # Normalize the concatenated output.
        x = self.batch_norm(x)
        # If activation is enabled, apply the ReLU activation.
        if self.activation:
            x = self.relu(x)
        # Return the final output.
        return x

# Build the model using the MixConv layers.
# The input shape is 224x224 with 3 color channels.
inputs = tf.keras.Input(shape=(224, 224, 3))
x = MixConv(32)(inputs)            # First MixConv layer with 32 filters.
x = MixConv(64)(x)                 # Second MixConv layer with 64 filters.
x = MixConv(128)(x)                # Third MixConv layer with 128 filters.
x = GlobalAveragePooling2D()(x)    # Apply global average pooling to reduce spatial dimensions.
x = Dense(128, activation="relu")(x)  # Add a fully connected layer with 128 units and ReLU activation.
x = Dense(len(CLASS_LABELS), activation="softmax")(x)  # Final output layer with softmax activation for classification.
model = tf.keras.Model(inputs, x)  # Create the model by specifying the inputs and outputs.

# Compile the model with the Adam optimizer, categorical crossentropy loss (for multi-class classification),
# and accuracy as the evaluation metric.
model.compile(optimizer="adam",
              loss="categorical_crossentropy",
              metrics=["accuracy"])

# Calculate the total number of training samples.
num_train_samples = sum([
    len([f for f in os.listdir(os.path.join(train_dir, cl)) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.webp'))])
    for cl in CLASS_LABELS
])
# Calculate the total number of validation samples.
num_val_samples = sum([
    len([f for f in os.listdir(os.path.join(validation_dir, cl)) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.webp'))])
    for cl in CLASS_LABELS
])
# Compute steps per epoch by dividing total samples by batch size and rounding up.
steps_per_epoch = math.ceil(num_train_samples / batch_size)
validation_steps = math.ceil(num_val_samples / batch_size)

# Train the model using the generators.
# The model will run for 10 epochs, using the training and validation generators.
history = model.fit(train_data,
                    steps_per_epoch=steps_per_epoch,
                    validation_data=validation_data,
                    validation_steps=validation_steps,
                    epochs=10,
                    verbose=2)

# Save the trained model to a file
model.save("my_model.h5")
print("Model saved as 'my_model.h5'")

# Define a function to preprocess an individual image and use the trained model to predict its class.
def preprocess_and_predict(image_path, model):
    # Open the image from the given path and convert it to RGB.
    img = Image.open(image_path).convert("RGB")
    # Get the original dimensions of the image.
    width, height = img.size
    # Determine the size of the square crop (using the smaller dimension).
    new_edge = min(width, height)
    # Calculate the coordinates for a centered crop.
    left = (width - new_edge) // 2
    top = (height - new_edge) // 2
    right = left + new_edge
    bottom = top + new_edge
    # Crop the image to a centered square.
    img = img.crop((left, top, right, bottom))
    # Resize the cropped image to 224x224 to match the model's input requirements.
    img = img.resize((224, 224), Image.Resampling.LANCZOS)

    # Convert the image to a numpy array and normalize pixel values to [0, 1].
    img_array = np.array(img) / 255.0
    # Expand dimensions so that the image shape becomes (1, 224, 224, 3) - a batch of one image.
    img_array = np.expand_dims(img_array, axis=0)

    # Get the model's predictions for the image.
    predictions = model.predict(img_array)
    # Identify the index of the highest predicted probability.
    predicted_idx = np.argmax(predictions[0])
    # Map the index to the corresponding class label.
    predicted_class = CLASS_LABELS[predicted_idx]
    # Calculate the confidence percentage for the predicted class.
    confidence = predictions[0][predicted_idx] * 100

    # Print the probability for each class.
    for label, prob in zip(CLASS_LABELS, predictions[0]):
        print(f"{label}: {prob * 100:.2f}%")

    # Display the image along with the predicted class and confidence.
    plt.imshow(img)
    plt.title(f"Predicted: {predicted_class} ({confidence:.2f}%)")
    plt.axis("off")
    plt.show()

    # Return the predicted class and confidence.
    return predicted_class, confidence

model = load_model("my_model.h5", custom_objects={'MixConv': MixConv})
# Optionally recompile (not necessary for prediction only)
model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])

# Create an interactive loop to allow the user to upload images and get predictions.
while True:
    # Prompt the user to upload an image for prediction.
    print("\nPlease upload an image for prediction.")
    # Open a file upload dialog in Google Colab.
    uploaded = files.upload()
    # Retrieve the file name of the uploaded image.
    image_path = list(uploaded.keys())[0]
    # Preprocess the image and predict its class using the trained model.
    predicted_class, confidence = preprocess_and_predict(image_path, model)
    # Print the predicted class and its confidence.
    print(f"Predicted Class: {predicted_class} ({confidence:.2f}%)")
    # Ask the user if they would like to upload another image.
    retry = input("\nDo you want to upload another image? (yes/no): ").strip().lower()
    # Exit the loop if the user does not enter "yes".
    if retry != "yes":
        print("Exiting. Thank you!")
        break